<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://leanderhb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leanderhb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-02T18:08:02+00:00</updated><id>https://leanderhb.github.io/feed.xml</id><title type="html">blank</title><subtitle>Pagina voor Leander Post, bevat thesis en wat ander werk. </subtitle><entry><title type="html">Grid based derivation</title><link href="https://leanderhb.github.io/update/2024/grid-based-derivation/" rel="alternate" type="text/html" title="Grid based derivation"/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/grid-based-derivation</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/grid-based-derivation/"><![CDATA[<p>If the plant is there and the food is there, there’s a certain probability of the plant uptaking the food. Also, plant have a certain chance of “duplicating”, by taking up the food. Besides that, the plants and the food do a random walk with a certain rate.</p> <ul> <li>plant eats food: \(P+F \xrightarrow{\alpha(P,F)} 2P\)</li> <li>plant dies: \(P\xrightarrow{\beta(P,F)} F\)</li> <li>plant hops: \(P_i \xrightarrow{d_P}P_{i\pm 1}\)</li> <li>food hops: \(F_i \xrightarrow{d_F}F_{i\pm 1}\) We do the classic thing of finding how the probability at a site changes:</li> </ul> <div>\[\begin{aligned} \frac{dP_i}{dt} = \underbrace{d_P\left( P_{i-1}+P_{i+1} \right)}_{\text{hopping to i}}-\underbrace{2d_PP_i}_{\text{hopping away}}+\underbrace{\alpha(P,F)PF}_{\text{plant duplicates}}-\underbrace{\beta(P,F) P}_{\text{plant dies}}\\ \frac{dF_i}{dt} = \underbrace{d_F\left( F_{i-1}+F_{i+1} \right)}_{\text{hopping to i}}-\underbrace{2d_FF_i}_{\text{hopping away}}-\underbrace{\alpha(P,F)PF}_{\text{food eaten}}+\underbrace{\beta(P,F) P}_{\text{new food}} \end{aligned}\]</div> <p>Taking the continuum limit, we get:</p> <div>\[\begin{aligned} P_t = D_P P_{xx}+\alpha(P,F)PF-\beta(P,F) P\\ F_t = D_F F_{xx}-\alpha(P,F)PF+\beta(P,F) P \end{aligned}\]</div> <p>More generally, with a bit of rescaling, we get:</p> <div>\[\begin{aligned} U_t&amp;=D U_{xx}+f(U,V)\\ V_t &amp;=V_{xx}-f(U,V) \end{aligned}\]</div> <p>with \(f\) now a general function of \(U\) and \(V\), now some generalized concentrations.</p> <p>This is called a reaction diffusion system, with conserved concentration :)</p> <h1 id="slides-for-fourier">Slides for fourier</h1> <p>Integral should have a real value. Then the left hand side should be real too. Then for positive, real \(\lambda\), we require that the square roots be real. If they aren’t, we can’t have positive, real \(\lambda\), which is interesting. This implies:</p> <div>\[ f_{V,r}&lt;f_{U,r},\quad f_{V,l}&lt;f_{U,l} \]</div> <p>is necessary for instability in the form of a real (non-oscillating) eigenvalue.</p> <p>Let’s check the stability around \((U_+,V_+)\). We linearize again:</p> <div>\[ \begin{aligned} u_t &amp;= \epsilon^2 u_{xx}+f_U(U_+,V_+)u+f_V(U_+,V_+)v\\ v_t &amp;= v_{xx}-f_U(U_+,V_+)u-f_V(U_+,V_+)v \end{aligned} \]</div> <p>A quick Fourier transform (we assume we’re far away enough from any structure that we can do this, other words, the slow reduced system), shows us that:</p> <div>\[\begin{aligned} \hat{u}_t = \epsilon^2(ik)^2\hat{u}+f_U \hat{u}+f_V\hat{v}\\ \hat{v}_t = (ik)^2\hat{v}-f_U\hat{u}-f_V\hat{v} \end{aligned} \]</div> <p>We now write this as:</p> <div>\[ \begin{pmatrix}\hat{u}\\\hat{v}\end{pmatrix}_t= \begin{pmatrix}f_U-\epsilon^2k^2&amp;f_V\\-f_U&amp;-f_V-k^2\end{pmatrix} \begin{pmatrix}\hat{u}\\\hat{v}\end{pmatrix} \]</div> <p>Which has determinant:</p> <div>\[\begin{aligned} \Delta\begin{pmatrix}f_U-\epsilon^2k^2&amp;f_V\\-f_U&amp;-f_V-k^2\end{pmatrix} &amp;=(f_U-\epsilon^2k^2)(-f_V-k^2)+f_Uf_V \\ &amp;= -f_Uk^2+\epsilon^2 k^2 f_V+ \epsilon^2k^4\\ &amp;= k^2(\epsilon^2 k^2+\epsilon^2 f_V-f_U) \end{aligned}\]</div> <p>We require the determinant to be positive for non-growing waves. We see that a bifurcation happens, if it does at all, for \(k=0\), since that makes the determinant closest to 0. Then we find that for stability of all wavenumbers, we need \(\epsilon^2 f_V-f_U&gt;0\). Next, we look at the trace, and find that</p> <div>\[ \operatorname{Tr}\begin{pmatrix}f_U-\epsilon^2k^2&amp;f_V\\-f_U&amp;-f_V-k^2\end{pmatrix} =f_U-\epsilon^2k^2-f_V-k^2 \]</div> <p>For stability, we require \(\operatorname{Tr}&lt;0\), so here we get another inequality for stability:</p> <div>\[ f_V-f_U&gt;0 \]</div>]]></content><author><name></name></author><summary type="html"><![CDATA[If the plant is there and the food is there, there’s a certain probability of the plant uptaking the food. Also, plant have a certain chance of “duplicating”, by taking up the food. Besides that, the plants and the food do a random walk with a certain rate. plant eats food: \(P+F \xrightarrow{\alpha(P,F)} 2P\) plant dies: \(P\xrightarrow{\beta(P,F)} F\) plant hops: \(P_i \xrightarrow{d_P}P_{i\pm 1}\) food hops: \(F_i \xrightarrow{d_F}F_{i\pm 1}\) We do the classic thing of finding how the probability at a site changes: \[\begin{aligned} \frac{dP_i}{dt} = \underbrace{d_P\left( P_{i-1}+P_{i+1} \right)}_{\text{hopping to i}}-\underbrace{2d_PP_i}_{\text{hopping away}}+\underbrace{\alpha(P,F)PF}_{\text{plant duplicates}}-\underbrace{\beta(P,F) P}_{\text{plant dies}}\\ \frac{dF_i}{dt} = \underbrace{d_F\left( F_{i-1}+F_{i+1} \right)}_{\text{hopping to i}}-\underbrace{2d_FF_i}_{\text{hopping away}}-\underbrace{\alpha(P,F)PF}_{\text{food eaten}}+\underbrace{\beta(P,F) P}_{\text{new food}} \end{aligned}\] Taking the continuum limit, we get: \[\begin{aligned} P_t=D_P P_{xx}+\alpha(P,F)PF-\beta(P,F) P\\ F_t=D_F F_{xx}-\alpha(P,F)PF+\beta(P,F) P \end{aligned}\] More generally, with a bit of rescaling, we get: \[\begin{aligned} U_t&amp;=D U_{xx}+f(U,V)\\ V_t &amp;=V_{xx}-f(U,V) \end{aligned}\] with \(f\) now a general function of \(U\) and \(V\), now some generalized concentrations.]]></summary></entry><entry><title type="html">Meeting tips</title><link href="https://leanderhb.github.io/update/2024/meeting-tips/" rel="alternate" type="text/html" title="Meeting tips"/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/meeting-tips</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/meeting-tips/"><![CDATA[<h3 id="arjen">arjen</h3> <p>dakje eraf: kleine letter pulsen: kleine mu op K, zodat vergelijken met wat we nu hebben, afgeleide naar mu: nieuwe eigenwaarde?</p> <h3 id="mazi">mazi</h3> <p>kpz simulate radial</p>]]></content><author><name></name></author><summary type="html"><![CDATA[arjen dakje eraf: kleine letter pulsen: kleine mu op K, zodat vergelijken met wat we nu hebben, afgeleide naar mu: nieuwe eigenwaarde?]]></summary></entry><entry><title type="html">3.1.11</title><link href="https://leanderhb.github.io/update/2024/3.1.11/" rel="alternate" type="text/html" title="3.1.11"/><published>2024-10-01T00:00:00+00:00</published><updated>2024-10-01T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/3.1.11</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/3.1.11/"><![CDATA[<p>Theorem 3.1.11. Assume that operator \(\mathcal{L}\) given in (3.1.1) is exponentially asymptotic with \(H^1(\mathbb{R})\) coefficients. Then \(\mathcal{L}\) is a relatively compact perturbation of the asymptotic operator \(\mathcal{L}_{\infty}\) given in (3.1.2). In particular,</p> <div>\[ \sigma_{\text {ess }}(\mathcal{L})=\left\{\lambda \in \mathbb{C} \mid i_{-}(\lambda) \neq \mathrm{i}_{+}(\lambda)\right\} \cup\left\{\lambda \in \mathbb{C} \mid \operatorname{dim} \mathbb{E}^{\mathrm{c}}\left(A_{ \pm}(\lambda)\right) \neq 0\right\} \]</div> <p>Moreover, for each \(\lambda \notin \sigma_{\text {ess }}(\mathcal{L})\), either \(\operatorname{dim}(\operatorname{ker}(\mathcal{L}-\lambda)) \neq 0\) or there exists \(C&gt;0\) such that</p> <div>\[ \left\|(\mathcal{L}-\lambda)^{-1} f\right\|_{H^n(\mathbb{R})} \leq C\|f\|_{L^2(\mathbb{R})} \]</div> <p>Proof. We consider only the case that the coefficients \(a_j(x)\) of \(\mathcal{L}\) are constant except on a common, compact interval \(I \subset \mathbb{R}\). To see that \(\mathcal{L}\) is a relatively compact perturbation of \(\mathcal{L}_{\infty}\), fix \(\lambda \in \rho\left(\mathcal{L}_{\infty}\right)\) and observe that the \(n\) th-order derivatives in \(\mathcal{L}_{\infty}-\mathcal{L}\) cancel, and setting aside the discontinuity at \(x=0\), we view the operator \(\mathcal{L}_{\infty}-\mathcal{L}\) as a piecewise map from \(H^n(\mathbb{R})\) into \(H^1\left(\mathbb{R}_{+}\right)\)and into \(H^1\left(\mathbb{R}_{-}\right)\). From Lemma 3.1 .10 we know that \(\left(\mathcal{L}_{\infty}-\lambda\right)^{-1}: L^2(\mathbb{R}) \mapsto H^n(\mathbb{R})\) is continuous, so that the composite map \(\left(\mathcal{L}_{\infty}-\mathcal{L}\right)\left(\mathcal{L}_{\infty}-\lambda\right)^{-1}: L^2(\mathbb{R}) \mapsto\) \(H^1\left(\mathbb{R}_{+}\right) \oplus H^1\left(\mathbb{R}_{-}\right)\)is continuous. Since the coefficients \(a_j(x)\) of \(\mathcal{L}\) are constant off \(I \subset \mathbb{R},\left(\mathcal{L}_{\infty}-\mathcal{L}\right)\left(\mathcal{L}_{\infty}-\lambda\right)^{-1}: L^2(\mathbb{R}) \mapsto H^1\left(I_{+}\right) \oplus H^1\left(I_{-}\right)\)where \(I_{-}=I \cap(-\infty, 0]\) and \(I_{+}=I \cap[0, \infty)\). In particular the map takes bounded sets to bounded sets. As bounded sets in \(H^1\left(I_{ \pm}\right)\) are equicontinuous and \(I_{ \pm}\)are compact, we deduce from the Arzela-Ascoli theorem that the operator \(\left(\mathcal{L}_{\infty}-\mathcal{L}\right)\left(\mathcal{L}_{\infty}-\lambda\right)^{-1}\) maps bounded sets of \(L^2(\mathbb{R})\) into precompact sets, and hence is compact. The characterization of the essential spectrum of \(\mathcal{L}\) follows from the Weyl’s essential spectrum Theorem 2.2.6 and Lemma 3.1.10. The statement (3.1.17) follows from the Fredholm alternative since \(\mathcal{L}-\lambda: H^n(\mathbb{R}) \rightarrow L^2(\mathbb{R})\) is Fredholm of index zero for \(\lambda \notin \sigma_{\text {ess }}(\mathcal{L})\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Theorem 3.1.11. Assume that operator \(\mathcal{L}\) given in (3.1.1) is exponentially asymptotic with \(H^1(\mathbb{R})\) coefficients. Then \(\mathcal{L}\) is a relatively compact perturbation of the asymptotic operator \(\mathcal{L}_{\infty}\) given in (3.1.2). In particular, \[ \sigma_{\text {ess }}(\mathcal{L})=\left\{\lambda \in \mathbb{C} \mid i_{-}(\lambda) \neq \mathrm{i}_{+}(\lambda)\right\} \cup\left\{\lambda \in \mathbb{C} \mid \operatorname{dim} \mathbb{E}^{\mathrm{c}}\left(A_{ \pm}(\lambda)\right) \neq 0\right\} \]]]></summary></entry><entry><title type="html">Proberen operator als perturbed</title><link href="https://leanderhb.github.io/update/2024/proberen-operator-als-perturbed/" rel="alternate" type="text/html" title="Proberen operator als perturbed"/><published>2024-10-01T00:00:00+00:00</published><updated>2024-10-01T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/proberen-operator-als-perturbed</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/proberen-operator-als-perturbed/"><![CDATA[<div>\[ \mathcal L = \mathcal L_0+\epsilon \mathcal L_1+\dots \]</div> <div>\[ \lambda\begin{pmatrix}u\\v\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ -\epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \begin{pmatrix}u\\v\end{pmatrix} \]</div> <p>expanding:</p> <div>\[ (\lambda_0+\epsilon^2\lambda_1+\dots)\begin{pmatrix}u_0+\epsilon^2 u_1+\dots\\ v_0+\epsilon^2 v_1+\epsilon^4 v_2+\dots\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ \epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \begin{pmatrix}u_0+\epsilon^2 u_1+\dots\\ v_0+\epsilon^2 v_1+\epsilon^4 v_2+\dots\end{pmatrix} \]</div> <p>collecting powers</p> <div>\[ \lambda\begin{pmatrix}u\\v\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ 0&amp;\partial_{\xi\xi} \end{pmatrix} \begin{pmatrix}u\\v\end{pmatrix} \]</div> <div>\[ \lambda\begin{pmatrix}v+\epsilon^2 u\\v\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ -\epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \begin{pmatrix}u\\v\end{pmatrix} \]</div> <p>dus:</p> <div>\[ \mathcal L = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ -\epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \]</div> <div>\[ v+\epsilon^2 u = \epsilon^2\partial_{\xi\xi} u+\partial_{\xi\xi}v+0 \]</div> <h1 id="expansion">expansion</h1> <div>\[ \lambda_0\begin{pmatrix}u_0\\ u_0+v_1\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;0 \\ 0&amp;\partial_{\xi\xi} \end{pmatrix} \begin{pmatrix}u_0\\u_0+ v_1\end{pmatrix} \]</div> <div>\[ \hat\lambda \begin{pmatrix}u_1\\ u_1+v_2\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U-f_V^*&amp;f_V^* \\ 0&amp;\partial_{\xi\xi} \end{pmatrix} \begin{pmatrix}u_1\\u_1+ v_2\end{pmatrix} \]</div> <p>Got it! You want to express the original system in the form of a first-order differential equation involving a 4-component vector. Let’s proceed.</p> <p>We start by rewriting the system in a way that uses a first-order form for (\partial_\xi). Define:</p> <p>[ \mathbf{p} = \begin{pmatrix} u \ v \ u_\xi \ v_\xi \end{pmatrix} ]</p> <p>Now, differentiate (\mathbf{p}) with respect to (\xi):</p> <p>[ \mathbf{p}<em>\xi = \begin{pmatrix} u</em>\xi \ v_\xi \ u_{\xi\xi} \ v_{\xi\xi} \end{pmatrix} ]</p> <p>We know the original equations involve second-order derivatives of (u) and (v). Rewrite them to relate these to the components of (\mathbf{p}):</p> <p>From:</p> <ol> <li>(\lambda u = (\partial_{\xi\xi} + f^<em>_U) u + f_V^</em> v)</li> </ol> <p>We can express the second derivative (u_{\xi\xi}) as:</p> <p>[ u_{\xi\xi} = \lambda u - f^<em>_U u - f_V^</em> v ]</p> <p>Similarly, from:</p> <ol> <li>(\lambda v = -\epsilon^2 f^<em>_U u + (\partial_{\xi\xi} - \epsilon^2 f_V^</em>) v)</li> </ol> <p>We get:</p> <p>[ v_{\xi\xi} = \lambda v + \epsilon^2 f^<em>_U u + \epsilon^2 f_V^</em> v ]</p> <p>Now, we can write this in matrix form as:</p> <p>[ \mathbf{p}_\xi = A \mathbf{p} ]</p> <p>where:</p> <p>[ A = \begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 <br/> 0 &amp; 0 &amp; 0 &amp; 1 <br/> \lambda - f^<em>_U &amp; -f_V^</em> &amp; 0 &amp; 0 <br/> \epsilon^2 f^<em>_U &amp; \lambda + \epsilon^2 f_V^</em> &amp; 0 &amp; 0 \end{pmatrix} ]</p> <p>Thus:</p> <p>[ \mathbf{p}_\xi = A \mathbf{p} ]</p> <p>This expresses the original system as a first-order differential equation in terms of a 4-component vector (\mathbf{p}).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[\[ \mathcal L=\mathcal L_0+\epsilon \mathcal L_1+\dots \] \[ \lambda\begin{pmatrix}u\\v\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ -\epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \begin{pmatrix}u\\v\end{pmatrix} \] expanding: \[ (\lambda_0+\epsilon^2\lambda_1+\dots)\begin{pmatrix}u_0+\epsilon^2 u_1+\dots\\ v_0+\epsilon^2 v_1+\epsilon^4 v_2+\dots\end{pmatrix} = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ \epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \begin{pmatrix}u_0+\epsilon^2 u_1+\dots\\ v_0+\epsilon^2 v_1+\epsilon^4 v_2+\dots\end{pmatrix} \]]]></summary></entry><entry><title type="html">Weyl voor niet-compact poging</title><link href="https://leanderhb.github.io/update/2024/weyl-voor-niet-compact-poging/" rel="alternate" type="text/html" title="Weyl voor niet-compact poging"/><published>2024-10-01T00:00:00+00:00</published><updated>2024-10-01T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/weyl-voor-niet-compact-poging</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/weyl-voor-niet-compact-poging/"><![CDATA[<div>\[ \mathcal L-\mathcal L_\infty \]</div> <div>\[ \mathcal L = \begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ -\epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \]</div> <p>Dus:</p> <div>\[ \mathcal L-\mathcal L_\infty = \begin{pmatrix}f^*_U(x)-&amp;f_V^*(x) \\ -f^*_U(x)&amp;f_V^*- f_V^*(x) \end{pmatrix} \]</div> <div>\[ \mathcal L-\mathcal L_\infty = \begin{pmatrix}a(x)-a_\pm&amp;b(x)-b_\pm \\ -a(x)+a_\pm&amp;-b(x)+b_\pm \end{pmatrix} \]</div> <div>\[ (\mathcal L-\mathcal L_\infty)(\mathcal L_\infty-\lambda)^{-1}(u,v) = \]</div> <div>\[ \|(\mathcal L_\infty-\lambda)^{-1}\| \leq C(\lambda)\|f\| \]</div> <p>Take weakest decay in \(\mathcal L-\mathcal L_\infty\), call it \(\alpha\), then:</p> <div>\[ \|(\mathcal L-\mathcal L_\infty)(\mathcal L_\infty-\lambda)^{-1}w \|\leq \|(\mathcal L-\mathcal L_\infty)(\mathcal L_\infty-\lambda)^{-1}w \| \]</div> <div>\[ \int |(\mathcal L-\mathcal L_\infty)(\mathcal L_\infty-\lambda)^{-1}w |^2dx= \]</div>]]></content><author><name></name></author><summary type="html"><![CDATA[\[ \mathcal L-\mathcal L_\infty \] \[ \mathcal L=\begin{pmatrix}\partial_{\xi\xi}+f^*_U&amp;f_V^* \\ -\epsilon^2 f^*_U&amp;\partial_{\xi\xi}-\epsilon^2 f_V^* \end{pmatrix} \] Dus: \[ \mathcal L-\mathcal L_\infty = \begin{pmatrix}f^*_U(x)-&amp;f_V^*(x) \\ -f^*_U(x)&amp;f_V^*- f_V^*(x) \end{pmatrix} \]]]></summary></entry><entry><title type="html">Slow eqns netjes</title><link href="https://leanderhb.github.io/update/2024/slow-eqns-netjes/" rel="alternate" type="text/html" title="Slow eqns netjes"/><published>2024-09-30T00:00:00+00:00</published><updated>2024-09-30T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/slow-eqns-netjes</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/slow-eqns-netjes/"><![CDATA[<p>For the slow equations, we introduce \(X=\epsilon x\). Then the (superslow) equations become:</p> <div>\[ \begin{aligned} \epsilon^2 \hat\lambda u &amp; = \epsilon^4u_{XX}+[f_U^* +\epsilon^2f_{UU}^* U_1^*+\epsilon^2f_{UV}^* (K_1^*-U_0^*)]u \\ &amp; +[f_V^* +\epsilon^2f_{VU}^* U_1^*+\epsilon^2f_{VU}^* (K^*_1-U_0^*)]v+\mathcal O(\epsilon^4) \\ \epsilon^2 \hat\lambda v &amp; = \epsilon^2 v_{XX}+[-f_U^* -\epsilon^2f_{UU}^* U_1^*-\epsilon^2f_{UV}^* (K_1^*-U_0^*)]u \\ &amp; +[-f_V^* -\epsilon^2f_{VU}^* U_1^*-\epsilon^2f_{VU}^* (K^*_1-U_0^*)]v+\mathcal O(\epsilon^4) \\ \end{aligned} \]</div> <p>Everything is exponentially close to constant. Both equations give at leading order:</p> <div>\[ u = -\frac{f_V^*}{f_U^*}v \]</div> <p>Now we add the equations and find at the next order:</p> <div>\[ \begin{aligned} \hat\lambda u+\hat\lambda v &amp; =v_{XX} \end{aligned} \]</div> <p>Then:</p> <div>\[ \begin{aligned} \hat\lambda\frac{f_U^*-f_V^*}{f_U^*} v &amp; = v_{XX} \end{aligned} \]</div> <p>Then:</p> <div>\[ v(X) = A\exp\left(\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }X\right)+B\exp\left(-\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }X\right) \]</div> <p>in the fast coordinate \(\xi= X/\epsilon^2\), we then get on the left:</p> <div>\[ v(\xi) = A\exp\left(\epsilon^2\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }\xi \right) \]</div>]]></content><author><name></name></author><summary type="html"><![CDATA[For the slow equations, we introduce \(X=\epsilon x\). Then the (superslow) equations become: \[ \begin{aligned} \epsilon^2 \hat\lambda u &amp; = \epsilon^4u_{XX}+[f_U^* +\epsilon^2f_{UU}^* U_1^*+\epsilon^2f_{UV}^* (K_1^*-U_0^*)]u \\ &amp; +[f_V^* +\epsilon^2f_{VU}^* U_1^*+\epsilon^2f_{VU}^* (K^*_1-U_0^*)]v+\mathcal O(\epsilon^4) \\ \epsilon^2 \hat\lambda v &amp; = \epsilon^2 v_{XX}+[-f_U^* -\epsilon^2f_{UU}^* U_1^*-\epsilon^2f_{UV}^* (K_1^*-U_0^*)]u \\ &amp; +[-f_V^* -\epsilon^2f_{VU}^* U_1^*-\epsilon^2f_{VU}^* (K^*_1-U_0^*)]v+\mathcal O(\epsilon^4) \\ \end{aligned} \] Everything is exponentially close to constant. Both equations give at leading order: \[ u=-\frac{f_V^*}{f_U^*}v \] Now we add the equations and find at the next order: \[ \begin{aligned} \hat\lambda u+\hat\lambda v &amp; =v_{XX} \end{aligned} \] Then: \[ \begin{aligned} \hat\lambda\frac{f_U^*-f_V^*}{f_U^*} v &amp; = v_{XX} \end{aligned} \] Then: \[ v(X) = A\exp\left(\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }X\right)+B\exp\left(-\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }X\right) \] in the fast coordinate \(\xi= X/\epsilon^2\), we then get on the left: \[ v(\xi) = A\exp\left(\epsilon^2\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }\xi \right) \]]]></summary></entry><entry><title type="html">Homoclinics</title><link href="https://leanderhb.github.io/update/2024/homoclinics/" rel="alternate" type="text/html" title="Homoclinics"/><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/homoclinics</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/homoclinics/"><![CDATA[<p>We start by slightly shifting \(K\) and end up with \(K+\delta\) with \(\delta\ll 1\). Then we find the corrections to \(U^\pm_{het},V^\pm_{het}\) as:</p> <div>\[\begin{aligned} U^\pm_{hom}=U^{\pm}_{het}+\delta \frac{f_V}{f_U}(U^\pm_{het},V^\pm_{het})\\ V^\pm_{hom}=V^\pm_{het}+\delta \end{aligned}\]</div> <p>Then the homoclinic Hamiltonian can be approximated (now for positive?? \(\delta\)):</p> <div>\[ H_{hom}(U)=\int_{U^-_{hom}}^Uf(W, K+\delta-\epsilon^2 W)dW \]</div> <p>Assuming for now that \(\delta\ll \epsilon\), we can partially expand:</p> <div>\[ H_{hom}(U)=\int_{U^{-}_{het}+\delta \frac{f_V^-}{f_U^-}}^Uf(W, K-\epsilon^2 W)+\delta f_V(W,K-\epsilon^2 W)dW \]</div> <p>Which we can reduce to:</p> <div>\[\begin{aligned} H_{hom}(U)=H_{het}(U)+\int_{U^{-}_{het}+\delta \frac{f_V^-}{f_U^-}}^{U^{-}_{het}}f(W, K-\epsilon^2 W)dW\\ +\delta \int_{U_{het}^-}^U f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2) \end{aligned}\]</div> <p>but \(f\) is close to zero, and will at most get to order \(\delta\) as long as it stays \(O(\delta)\) from \(U_{het}^-\), so also this term can be neglected. So we’re left with:</p> <div>\[ \begin{aligned} H_{hom}(U,\delta)=H_{het}(U)+\delta \int_{U_{het}^-}^U f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2) \end{aligned} \]</div> <p>Now by similar argument we find that:</p> <div>\[ \begin{aligned} H_{hom}(U^+_{hom},\delta)&amp;=H_{het}(U^+_{hom})+\delta \int_{U_{het}^-}^{U^+_{hom}} f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2)\\ &amp;=\delta \int_{U_{het}^-}^{U^+_{het}} f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2) \end{aligned} \]</div> <p>Cool, now we can estimate how far away the turning point of the homoclinic orbit is, by using a quadratic approximation of \(H\) around \(U^+_{hom}\). We find:</p> <div>\[ H(U+\mu) = H(U)+\mu H_U(U)+\mu^2/2 H_{UU}(U) \]</div> <p>We know the value of \(H(U)\) by above analysis, and know that \(H_U\) there is zero (top of the peak), and we know that \(H_{UU}=\partial_U f(U,K+\delta-\epsilon^2 U)\). So a leading order approximation of it is given by:</p> <div>\[ H_{UU}=f_U(U^+_{het},V^+_{het}) \]</div> <p>So then:</p> <div>\[ H(U+\mu_0)=0\implies \mu_0 = \pm\sqrt{2\delta \frac{1}{f_U^+}\int_{U_{het}^-}^{U^+_{het}} f_V(W,K-\epsilon^2 W)dW }+\mathcal O(\delta) \]</div> <p>let’s connect this to the rest of the theory to get an estimate of the width of the front:</p> <div>\[ \epsilon^2 U_{xx}=f_U^+U \]</div> <p>is the equation close to the turning point. Then we solve it and find:</p> <div>\[ U(x) = A\exp\left(\frac{\sqrt c}{\epsilon}x\right)+B\exp\left(-\frac{\sqrt c}{\epsilon}x\right) \]</div> <p>Solving for the initial conditions, we use \(U(0)=\mu_0,U_x(0)=0\) and get:</p> <div>\[ U(x) = \frac{\mu_0}{2}\left(\exp\left(\frac{\sqrt c}{\epsilon}x\right)+\exp\left(-\frac{\sqrt c}{\epsilon}x\right)\right) \]</div> <p>but the expression in brackets is actually a known quantity: the cosh.</p> <div>\[ U(x) = \mu_0\cosh\left(\frac{\sqrt c}{\epsilon}x\right) \]</div> <p>Let’s find when \(U(x)\) becomes order 1:</p> <div>\[ \frac1{\mu_0} = \cosh\left(\frac{\sqrt c}{\epsilon}x\right) \]</div> <p>Solving for \(x\):</p> <div>\[ x=\frac{\epsilon}{\sqrt c}\cosh^{-1}\left(\frac1{\mu_0}\right) \]</div> <p>Since \(\mu_0\ll 1\), we can approximate this by:</p> <div>\[ x=\frac{\epsilon}{\sqrt c}\ln\left(\frac2{\mu_0}\right) \]</div> <p>Expanding the \(\ln\):</p> <div>\[ x=\frac{\epsilon}{\sqrt c}\left(\ln 2-\ln\mu_0\right) \]</div> <p>Of course, we know that \(\ln 2\ll\ln\mu_0\), so to leading order, we get for the width:</p> <div>\[ W=-\frac{2\epsilon}{\sqrt c}\ln\mu_0 \]</div> <p>again tossing away non-leading terms, we then also get:</p> <div>\[ W=-\frac{2\epsilon}{\sqrt c}\ln{\sqrt\delta}=-\frac{\epsilon}{\sqrt c}\ln{\delta} \]</div> <p>So then:</p> <div>\[ \delta = \exp\left(-\frac{\sqrt c}{\epsilon }W \right) \]</div>]]></content><author><name></name></author><summary type="html"><![CDATA[We start by slightly shifting \(K\) and end up with \(K+\delta\) with \(\delta\ll 1\). Then we find the corrections to \(U^\pm_{het},V^\pm_{het}\) as: \[\begin{aligned} U^\pm_{hom}=U^{\pm}_{het}+\delta \frac{f_V}{f_U}(U^\pm_{het},V^\pm_{het})\\ V^\pm_{hom}=V^\pm_{het}+\delta \end{aligned}\] Then the homoclinic Hamiltonian can be approximated (now for positive?? \(\delta\)): \[ H_{hom}(U)=\int_{U^-_{hom}}^Uf(W, K+\delta-\epsilon^2 W)dW \] Assuming for now that \(\delta\ll \epsilon\), we can partially expand: \[ H_{hom}(U)=\int_{U^{-}_{het}+\delta \frac{f_V^-}{f_U^-}}^Uf(W, K-\epsilon^2 W)+\delta f_V(W,K-\epsilon^2 W)dW \] Which we can reduce to: \[\begin{aligned} H_{hom}(U)=H_{het}(U)+\int_{U^{-}_{het}+\delta \frac{f_V^-}{f_U^-}}^{U^{-}_{het}}f(W, K-\epsilon^2 W)dW\\ +\delta \int_{U_{het}^-}^U f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2) \end{aligned}\] but \(f\) is close to zero, and will at most get to order \(\delta\) as long as it stays \(O(\delta)\) from \(U_{het}^-\), so also this term can be neglected. So we’re left with: \[ \begin{aligned} H_{hom}(U,\delta)=H_{het}(U)+\delta \int_{U_{het}^-}^U f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2) \end{aligned} \] Now by similar argument we find that: \[ \begin{aligned} H_{hom}(U^+_{hom},\delta)&amp;=H_{het}(U^+_{hom})+\delta \int_{U_{het}^-}^{U^+_{hom}} f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2)\\ &amp;=\delta \int_{U_{het}^-}^{U^+_{het}} f_V(W,K-\epsilon^2 W)dW +\mathcal O(\delta^2) \end{aligned} \] Cool, now we can estimate how far away the turning point of the homoclinic orbit is, by using a quadratic approximation of \(H\) around \(U^+_{hom}\). We find: \[ H(U+\mu) = H(U)+\mu H_U(U)+\mu^2/2 H_{UU}(U) \] We know the value of \(H(U)\) by above analysis, and know that \(H_U\) there is zero (top of the peak), and we know that \(H_{UU}=\partial_U f(U,K+\delta-\epsilon^2 U)\). So a leading order approximation of it is given by: \[ H_{UU}=f_U(U^+_{het},V^+_{het}) \] So then: \[ H(U+\mu_0)=0\implies \mu_0 = \pm\sqrt{2\delta \frac{1}{f_U^+}\int_{U_{het}^-}^{U^+_{het}} f_V(W,K-\epsilon^2 W)dW }+\mathcal O(\delta) \] let’s connect this to the rest of the theory to get an estimate of the width of the front: \[ \epsilon^2 U_{xx}=f_U^+U \] is the equation close to the turning point. Then we solve it and find: \[ U(x) = A\exp\left(\frac{\sqrt c}{\epsilon}x\right)+B\exp\left(-\frac{\sqrt c}{\epsilon}x\right) \] Solving for the initial conditions, we use \(U(0)=\mu_0,U_x(0)=0\) and get: \[ U(x) = \frac{\mu_0}{2}\left(\exp\left(\frac{\sqrt c}{\epsilon}x\right)+\exp\left(-\frac{\sqrt c}{\epsilon}x\right)\right) \] but the expression in brackets is actually a known quantity: the cosh. \[ U(x) = \mu_0\cosh\left(\frac{\sqrt c}{\epsilon}x\right) \] Let’s find when \(U(x)\) becomes order 1: \[ \frac1{\mu_0} = \cosh\left(\frac{\sqrt c}{\epsilon}x\right) \] Solving for \(x\): \[ x=\frac{\epsilon}{\sqrt c}\cosh^{-1}\left(\frac1{\mu_0}\right) \] Since \(\mu_0\ll 1\), we can approximate this by: \[ x=\frac{\epsilon}{\sqrt c}\ln\left(\frac2{\mu_0}\right) \] Expanding the \(\ln\): \[ x=\frac{\epsilon}{\sqrt c}\left(\ln 2-\ln\mu_0\right) \] Of course, we know that \(\ln 2\ll\ln\mu_0\), so to leading order, we get for the width: \[ W=-\frac{2\epsilon}{\sqrt c}\ln\mu_0 \] again tossing away non-leading terms, we then also get: \[ W=-\frac{2\epsilon}{\sqrt c}\ln{\sqrt\delta}=-\frac{\epsilon}{\sqrt c}\ln{\delta} \] So then: \[ \delta = \exp\left(-\frac{\sqrt c}{\epsilon }W \right) \]]]></summary></entry><entry><title type="html">Thesis handige thms</title><link href="https://leanderhb.github.io/update/2024/thesis-handige-thms/" rel="alternate" type="text/html" title="Thesis handige thms"/><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/thesis-handige-thms</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/thesis-handige-thms/"><![CDATA[<h1 id="thms">thms</h1> <p><img src="/assets/images/Pasted image 20240215153011.png" alt="Image"/> (Abel’s Thm?) <img src="/assets/images/Pasted image 20240215155740.png" alt="Image"/></p> <p>Homogeneous sols: <img src="/assets/images/Pasted image 20240215160407.png" alt="Image"/></p> <p><img src="/assets/images/Pasted image 20240216164924.png" alt="Image"/></p> <p>essential spectrum ligt op negative real part: <img src="/assets/images/Pasted image 20240216165812.png" alt="Image"/>vul in dat \(a_0^\pm\) de slopes zijn van de intersection reactive NC en flux-balance lijn!</p> <p><img src="/assets/images/Pasted image 20240216180438.png" alt="Image"/></p> <p><img src="/assets/images/Pasted image 20240216180512.png" alt="Image"/> lol al het werk eerder in 1 slide</p> <p><img src="/assets/images/Pasted image 20240221151840.png" alt="Image"/> <img src="/assets/images/Pasted image 20240221152222.png" alt="Image"/></p> <p><img src="/assets/images/Pasted image 20240221152707.png" alt="Image"/></p> <p>Boundary layer thickness: <img src="/assets/images/Pasted image 20240227160002.png" alt="Image"/>Dus dat geeft een estimate op hoe dik we die willen hebben.</p> <div>\[ d \sim \sqrt\delta \]</div> <p><img src="/assets/images/Pasted image 20240311125859.png" alt="Image"/></p> <p>niet bewezen?? <img src="/assets/images/Pasted image 20240312153150.png" alt="Image"/></p> <p>Dit is ongeveer wat ik deed met mn staarten? <img src="/assets/images/Pasted image 20240315141901.png" alt="Image"/></p> <p>laten zien dat snelheid \(\epsilon^2\): <img src="/assets/images/Pasted image 20240326171327.png" alt="Image"/></p> <p>waarom BL? <img src="/assets/images/Pasted image 20240409113124.png" alt="Image"/></p> <p><img src="/assets/images/Pasted image 20240603143347.png" alt="Image"/></p> <p><img src="/assets/images/Pasted image 20240909153206.png" alt="Image"/></p> <p>arjens notes <img src="/assets/images/Pasted image 20240912185201.png" alt="Image"/></p> <p><img src="/assets/images/Pasted image 20240918134916.png" alt="Image"/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[thms (Abel’s Thm?)]]></summary></entry><entry><title type="html">Update text natuurkunde</title><link href="https://leanderhb.github.io/update/2024/update-text-natuurkunde/" rel="alternate" type="text/html" title="Update text natuurkunde"/><published>2024-09-16T00:00:00+00:00</published><updated>2024-09-16T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/update-text-natuurkunde</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/update-text-natuurkunde/"><![CDATA[<p>Hi! I’ve got an update, I’ve been working on the stability part of my thesis for a couple of months now, and feel I’ve found a solution to it. It’s rather technical, but a short summary: I formalize an eigenvalue problem by linearizing around a steady state solution (front), the eigenvalues being the decay/growth rate (positive: growing, negative: decaying). The eigenfunctions are the perturbations that will either grow/decay based on the eigenvalue. Solving for the eigenvalues is a bit tricky. Solving exactly is infeasible, so I find approximate solutions, and some functional analysis theory then tells me that under certain conditions, these approximate solutions are close enough to guarantee stability (hence “physical solutions”, things you see in nature, that don’t decay by themselves). This has a cool basis both in geometry (stable/unstable manifolds) and functional analysis (the eigenvalue problem with Sturm-Liouville type operators). To find these approximate solutions, I cut up the real axis into three parts: two “slow” regions, and one “fast” region. This is similar to what you do in boundary layer problems in, for example, fluid dynamics. Or two-timescale problems in neuroscience. I then solve the simplified solutions perturbatively in all regimes (left slow, center fast, right slow). Then to have a smooth solution (which I require as solutions to the PDE), I match amplitude of the boundaries of these regimes, and I match the first order derivative. The main work I’ve done is finding the values to match at, and finding the right perturbations (I was stuck at the wrong order of expansions for a while). This is basically a whole lot of calculus and Fredholm theory to find that some integrals should equal each-other. I’ll link two documents, the first of which I think highlights the problem more completely, but goes wrong at the stability analysis (the wrong order of expansion), the second has the right expansion, and my results, but lacks the context. I completely understand if you don’t want to dive into these at this moment. If you do however, I’ll draft up a more readable document, but if no-one is going to read it anyways, I’ll save my time. So let me know if you want a document with some context. If you like, I would love to drop by some time and explain with a presentation etc, let me know if you’re interested/have time. We agreed that I’d reach out if I needed help, so far that hasn’t been the case. The next part I will study will probably be similar, and I think won’t take extremely long, now I’ve gotten a good feeling for this first part, but the part after that deals with some topics similar to surface tension, at that point I’d love to chat a bit, as I think the physics insight would be very helpful.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! I’ve got an update, I’ve been working on the stability part of my thesis for a couple of months now, and feel I’ve found a solution to it. It’s rather technical, but a short summary: I formalize an eigenvalue problem by linearizing around a steady state solution (front), the eigenvalues being the decay/growth rate (positive: growing, negative: decaying). The eigenfunctions are the perturbations that will either grow/decay based on the eigenvalue. Solving for the eigenvalues is a bit tricky. Solving exactly is infeasible, so I find approximate solutions, and some functional analysis theory then tells me that under certain conditions, these approximate solutions are close enough to guarantee stability (hence “physical solutions”, things you see in nature, that don’t decay by themselves). This has a cool basis both in geometry (stable/unstable manifolds) and functional analysis (the eigenvalue problem with Sturm-Liouville type operators). To find these approximate solutions, I cut up the real axis into three parts: two “slow” regions, and one “fast” region. This is similar to what you do in boundary layer problems in, for example, fluid dynamics. Or two-timescale problems in neuroscience. I then solve the simplified solutions perturbatively in all regimes (left slow, center fast, right slow). Then to have a smooth solution (which I require as solutions to the PDE), I match amplitude of the boundaries of these regimes, and I match the first order derivative. The main work I’ve done is finding the values to match at, and finding the right perturbations (I was stuck at the wrong order of expansions for a while). This is basically a whole lot of calculus and Fredholm theory to find that some integrals should equal each-other. I’ll link two documents, the first of which I think highlights the problem more completely, but goes wrong at the stability analysis (the wrong order of expansion), the second has the right expansion, and my results, but lacks the context. I completely understand if you don’t want to dive into these at this moment. If you do however, I’ll draft up a more readable document, but if no-one is going to read it anyways, I’ll save my time. So let me know if you want a document with some context. If you like, I would love to drop by some time and explain with a presentation etc, let me know if you’re interested/have time. We agreed that I’d reach out if I needed help, so far that hasn’t been the case. The next part I will study will probably be similar, and I think won’t take extremely long, now I’ve gotten a good feeling for this first part, but the part after that deals with some topics similar to surface tension, at that point I’d love to chat a bit, as I think the physics insight would be very helpful.]]></summary></entry><entry><title type="html">Erg cool</title><link href="https://leanderhb.github.io/update/2024/erg-cool/" rel="alternate" type="text/html" title="Erg cool"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://leanderhb.github.io/update/2024/erg-cool</id><content type="html" xml:base="https://leanderhb.github.io/update/2024/erg-cool/"><![CDATA[<p>We start with the set of equations:</p> <div>\[\begin{aligned} U_t &amp;= \epsilon^2U_{xx}+f(U,V)\\ V_t &amp;= V_{xx}-f(U,V) \end{aligned} \]</div> <p>By previous analysis, we know that we get a 1 parameter family in \(K\) of pulses and fronts described by a Hamiltonian system in \(U\), for which there is a specific \(K^*\) for which we obtain a front solution. Take this \(K^*\) and the front \((U^*,V^*)=(U^*,K^*-\epsilon^2 U^*)\) that comes with it. We can expand these values and obtain:</p> <div>\[ \begin{aligned} K^*&amp;=K^*_0+\epsilon^2K_1^*+\mathcal O(\epsilon^4)\\ U^*&amp;=U^*_0+\epsilon^2U_1^*+\mathcal O(\epsilon^4)\\ V^*&amp;=V^*_0+\epsilon^2V_1^*+\mathcal O(\epsilon^4) = K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*+\mathcal O(\epsilon^4) \end{aligned} \]</div> <p>Let’s perturb the system to check stability, take \(\delta\) smaller than all system scales, and define:</p> <div>\[\begin{aligned} U(\xi) &amp;= U^*(\xi)+\delta \exp(\lambda t)u(\xi)\\ V(\xi) &amp;= V^*(\xi)+\delta \exp(\lambda t)v(\xi) \end{aligned}\]</div> <p>Inserting this into the PDE and throwing out terms of order \(\delta^2\), we get the linearized equations:</p> <div>\[ \begin{aligned} \lambda u &amp;= \epsilon^2u_{xx}+f_U(U^*,V^*)u+f_V(U^*,V^*)v\\ \lambda v&amp;= v_{xx}-f_U(U^*,V^*)u-f_V(U^*,V^*)v \end{aligned} \]</div> <p>next, expand \(u,v\) as:</p> <div>\[\begin{aligned} u&amp;=u_0+\epsilon^2 u_1+\dots\\ v&amp;=v_0+\epsilon^2 v_1+\epsilon^4 v_2\dots \end{aligned}\]</div> <h1 id="expanding-it-all">Expanding it all</h1> <p>Completely expanded we find (check order error lieve makker):</p> <div>\[ \begin{aligned} \epsilon^2 \hat\lambda (u_0+\epsilon^2 u_1) &amp;= \epsilon^2(u_0+\epsilon^2 u_1)_{xx}+[f_U^* +\epsilon^2f_{UU}^* U_1^*+\epsilon^2f_{UV}^* (K_1^*-U_0^*)](u_0+\epsilon^2 u_1)\\ &amp;+[f_V^* +\epsilon^2f_{VU}^* U_1^*+\epsilon^2f_{VU}^* (K^*_1-U_0^*)](v_0+\epsilon^2 v_1+\epsilon^4 v_2)+\mathcal O(\epsilon^6)\\ \epsilon^2 \hat\lambda (v_0+\epsilon^2 v_1)&amp;= (v_0+\epsilon^2 v_1+\epsilon^4 v_2)_{xx}+[-f_U^* -\epsilon^2f_{UU}^* U_1^*-\epsilon^2f_{UV}^* (K_1^*-U_0^*)](u_0+\epsilon^2 u_1)\\ &amp;+[-f_V^* -\epsilon^2f_{VU}^* U_1^*-\epsilon^2f_{VU}^* (K^*_1-U_0^*)](v_0+\epsilon^2 v_1+\epsilon^4 v_2)+\mathcal O(\epsilon^6)\\ \end{aligned} \]</div> <p>adding these two we get:</p> <div>\[ \begin{aligned} \epsilon^2 \hat\lambda (u_0+\epsilon^2 u_1+v_0+\epsilon^2 v_1) &amp;= (\epsilon^2u_0+\epsilon^4 u_1+v_0+\epsilon^2 v_1+\epsilon^4 v_2)_{xx}+O(\epsilon^6)\\ \end{aligned} \]</div> <p>and in the fast coordinate:</p> <div>\[ \begin{aligned} \epsilon^4\hat\lambda (u_0+v_0) &amp;= (\epsilon^2u_0+\epsilon^4 u_1+v_0+\epsilon^2 v_1+\epsilon^4 v_2)_{\xi\xi}+O(\epsilon^6)\\ \end{aligned} \]</div> <p>We can now check the equations of all orders:</p> <div>\[ \begin{aligned} O(1):\quad 0 &amp;= (v_0)_{\xi\xi}\\ O(\epsilon^2):\quad0 &amp;= (u_0+ v_1)_{\xi\xi}\\ O(\epsilon^4):\quad \hat\lambda (u_0+v_0) &amp;= ( u_1+ v_2)_{\xi\xi}\\ \end{aligned} \]</div> <p>Via Fredholm alternative, and \(O(1)\) \(u\) equations, we need \(v_0=0\). Then this reduces to:</p> <div>\[ \begin{aligned} O(1):\quad v_0&amp;=0\\ O(\epsilon^2):\quad0 &amp;= (u_0+ v_1)_{\xi\xi}\\ O(\epsilon^4):\quad \hat\lambda u_0 &amp;= ( u_1+ v_2)_{\xi\xi}\\ \end{aligned} \]</div> <p>Furthermore, we know that \(u_0=U^*_{0,\xi}\) solves the equation, so we can assume we know \(u_0\). Then we also know \(v_1 = C-u_0\). Unknowns are then \(u_1,v_2\), which will in essence be the first order correction to the translational mode. The question is if this correction will then push \(\lambda\) into the positive or negative region. The equation for \(u_1\) is the following:</p> <div>\[ \hat\lambda u_0 = u_{1,\xi\xi}+f_U^* u_1+[ f_{UU}^* U_1^*+f_{UV}^* (K_1^*-U_0^*)]u_0+ v_1f_V^* \]</div> <h1 id="getting-rid-of-the-second-order-derivatives">Getting rid of the second order derivatives</h1> <div>\[ \begin{aligned} 0 &amp;= \epsilon^2U^*_{xx}+f(U^*,V^*)\\ 0 &amp;= V^*_{xx}-f(U^*,V^*) \end{aligned} \]</div> <p>We expand with:</p> <div>\[ \begin{aligned} K^*&amp;=K^*_0+\epsilon^2K_1^*+\mathcal O(\epsilon^4)\\ U^*&amp;=U^*_0+\epsilon^2U_1^*+\mathcal O(\epsilon^4)\\ V^*&amp;=V^*_0+\epsilon^2V_1^*+\mathcal O(\epsilon^4) = K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*+\mathcal O(\epsilon^4) \end{aligned} \]</div> <p>Then we get:</p> <div>\[ \begin{aligned} 0 &amp;= \epsilon^2(U^*_0+\epsilon^2U_1^*)_{xx}+f(U^*_0+\epsilon^2U_1^*,K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*)\\ 0 &amp;= (K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*)_{xx}-f(U^*_0+\epsilon^2U_1^*,K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*) \end{aligned} \]</div> <p>we can add the equations and obtain:</p> <div>\[ \begin{aligned} 0 &amp;= \epsilon^2(U^*_0+\epsilon^2U_1^*)_{xx}+(K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*)_{xx}\\ \end{aligned} \]</div> <p>In the fast coordinates:</p> <div>\[ \begin{aligned} 0 &amp;= (U^*_0+\epsilon^2U_1^*)_{\xi\xi}+f(U^*_0+\epsilon^2U_1^*,K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*)\\ 0 &amp;= (K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*)_{\xi\xi}-\epsilon^2f(U^*_0+\epsilon^2U_1^*,K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*) \end{aligned} \]</div> <p>expanding:</p> <div>\[ \begin{aligned} f(U^*_0+\epsilon^2U_1^*,K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*) = \\ f(U^*_0,K^*_0)+\epsilon^2f_U(U^*_0,K^*_0)U_1^*+\epsilon^2f_V(U^*_0,K^*_0)( K_1^*- U_0^*) \end{aligned} \]</div> <p>This gives the equations:</p> <div>\[ \begin{aligned} 0 &amp;= (U^*_0+\epsilon^2U_1^*)_{\xi\xi}+f(U^*_0,K^*_0)+\epsilon^2f_U(U^*_0,K^*_0)U_1^*+\epsilon^2f_V(U^*_0,K^*_0)( K_1^*- U_0^*)\\ 0 &amp;= (K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*)_{\xi\xi}-\epsilon^2f(U^*_0+\epsilon^2U_1^*,K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*) \end{aligned} \]</div> <p>collecting for real (second eqn not important?)</p> <div>\[ \begin{aligned} 0 &amp;= (U^*_0)_{\xi\xi}+f(U^*_0,K^*_0)\\ 0 &amp;= (U_1^*)_{\xi\xi}+f_U(U^*_0,K^*_0)U_1^*+f_V(U^*_0,K^*_0)( K_1^*- U_0^*)\\ \end{aligned} \]</div> <p>Fredholm tells us:</p> <div>\[ K_1^* = \frac{\int U^*_0 U^*_{0,\xi} f_V^*}{\int U^*_{0,\xi} f_V^*} \]</div> <p>Let’s take the derivative of the second equation:</p> <div>\[ \begin{aligned} 0 &amp;= \partial_\xi(U_1^*)_{\xi\xi} \\ &amp;+\partial_\xi (f_U(U^*_0,K^*_0)U_1^*)\\ &amp;+\partial_\xi(f_V(U^*_0,K^*_0)( K_1^*- U_0^*))\\ &amp;= (U_1^*)_{\xi\xi\xi} \\ &amp;+ f_{UU}(U^*_0,K^*_0)U^*_{0,\xi}U_1^*+f_U(U^*_0,K^*_0)U_{1,\xi}^*\\ &amp;+f_{UV}(U^*_0,K^*_0)U^*_{0,\xi}( K_1^*- U_0^*)+f_V(U^*_0,K^*_0)( - U_{0,\xi}^*)\\ \end{aligned} \]</div> <p>Now we look at our original equation for \(u_1\):</p> <div>\[ \hat\lambda u_0 = u_{1,\xi\xi}+f_U^* u_1+[ f_{UU}^* U_1^*+f_{UV}^* (K_1^*-U_0^*)]u_0+ v_1f_V^* \]</div> <p>and we recognize a lot of terms. Let’s write our derivative-equation as follows:</p> <div>\[ \begin{aligned} 0 &amp;= (U_1^*)_{\xi\xi\xi}+f_U^*U_{1,\xi}^* + [f_{UU}^* U_1^*+f_{UV}^*( K_1^*- U_0^*)-f_V^*] u_0\\ \end{aligned} \]</div> <p>Finally, note that \(v_1=C-u_0\) as we found earlier. Then we can replace a lot of terms and end up with:</p> <div>\[ \hat\lambda u_0 = u_{1,\xi\xi}+f_U^* u_1-[U_{1,\xi\xi\xi}^*+f_U^*U_{1,\xi}^*]+Cf_V^* \]</div> <p>This seems nice, we got rid of the annoying second derivatives and we don’t need \(K_1\) anymore. However, we are now stuck with \(U_1\).</p> <p>However, we can use the fredholm integrability criterion and get:</p> <div>\[ \langle u_0,\hat\lambda u_0+ [U_{1,\xi\xi\xi}^*+f_U^*U_{1,\xi}^*]\rangle = C\int f_V^*u_0 d\xi \]</div> <p>writing out int:</p> <div>\[ \int [\hat\lambda u_0u_0+ u_0U_{1,\xi\xi\xi}^*+f_U^*U_{1,\xi}^*u_0] = C\int f_V^*u_0 d\xi \]</div> <p>partially integrating the middle term twice (boundary is zero? ?) Aaah twee keer min 1 jaaaa mooi</p> <div>\[ \int [\hat\lambda u_0^2+ u_{0,\xi\xi}U_{1,\xi}^*+f_U^*U_{1,\xi}^*u_0] = C\int f_V^*u_0 d\xi \]</div> <p>using eqn for \(u_{0,\xi\xi}\), namely \(0= (u_0)_{\xi\xi}+f_U^*u_0\), we get:</p> <div>\[ \int [\hat\lambda u_0^2+ (-f_U^* u_0)U_{1,\xi}^*+f_U^*U_{1,\xi}^*u_0] = C\int f_V^*u_0 d\xi \]</div> <p>The terms neatly cancel, leaving:</p> <div>\[ \int [\hat\lambda u_0^2] = C\int f_V^*u_0 d\xi \]</div> <p>So we find \(C\) to be:</p> <div>\[ C=\hat\lambda\frac{\int u_0^2}{\int f_V^*u_0 d\xi} \]</div> <p>Which is exactly the same again as \(\nu&lt;2\), but needs a whole lot of extra steps haha</p> <h1 id="slow-equations">Slow equations</h1> <div>\[ \begin{aligned} \epsilon^2 \hat\lambda u &amp;= \epsilon^2u_{xx}+[f_U^* +\epsilon^2f_{UU}^* U_1^*+\epsilon^2f_{UV}^* (K_1^*-U_0^*)]u\\ &amp;+[f_V^* +\epsilon^2f_{VU}^* U_1^*+\epsilon^2f_{VU}^* (K^*_1-U_0^*)]v+\mathcal O(\epsilon^4)\\ \epsilon^2 \hat\lambda v&amp;= v_{xx}+[-f_U^* -\epsilon^2f_{UU}^* U_1^*-\epsilon^2f_{UV}^* (K_1^*-U_0^*)]u\\ &amp;+[-f_V^* -\epsilon^2f_{VU}^* U_1^*-\epsilon^2f_{VU}^* (K^*_1-U_0^*)]v+\mathcal O(\epsilon^4)\\ \end{aligned} \]</div> <p>We find at leading order:</p> <div>\[ u = -\frac{f_V^*}{f_U^*}v+\mathcal O(\epsilon^2) \]</div> <p>Now we add the equations and find:</p> <div>\[ \begin{aligned} \epsilon^2 \hat\lambda u+\epsilon^2 \hat\lambda v &amp;= \epsilon^2u_{xx}+v_{xx} \end{aligned} \]</div> <p>The second order derivative is simply:</p> <div>\[ u_{xx} = \frac{f_V^*}{f_U^*}v_{xx} \]</div> <p>Is it??? Probably smooth enough, but seems wrong somehow, but yea everything is exponentially close to constant so this should definitely work. Then to leading order we’re left with:</p> <div>\[ \begin{aligned} \epsilon^2\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} v &amp;= v_{xx} \end{aligned} \]</div> <p>Then:</p> <div>\[ v = A\exp\left(\epsilon\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }x\right)+B\exp\left(-\epsilon\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }x\right) \]</div> <p>in fast coordinate \(x = \epsilon\xi\), we then get on the left:</p> <div>\[ v = A\exp\left(\epsilon^2\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} }\xi \right) \]</div> <p>We should match in amplitude to the inner solution, where leading order at the edge, we have \(v=\epsilon^2v_1=\epsilon^2C\), so by continuity, \(A=\epsilon^2 C\). And therefore the derivative at the edge:</p> <div>\[ v_\xi(\epsilon^\sigma) = C\epsilon^4\sqrt{\hat\lambda\frac{f_U^*-f_V^*}{f_U^*} } \]</div> <p>left use the shorthand \(\rho_l = \frac{f_U^*-f_V^*}{f_U^*}\) on the left and similarly for \(\rho_r\). Then we get:</p> <div>\[ \Delta_{slow}(v+\epsilon^2 u)_\xi = \Delta_{slow}v_\xi+O(\epsilon^2) = -C\epsilon^4\sqrt{\hat\lambda}(\sqrt{\rho_r}+\sqrt{\rho_l}) \]</div> <p>Very nice, let’s fill \(C\) in too:</p> <div>\[ \Delta_{slow}(v+\epsilon^2 u)_\xi = -\hat\lambda\frac{\int u_0^2}{\int f_V^*u_0 d\xi}\epsilon^4\sqrt{\hat\lambda}(\sqrt{\rho_r}+\sqrt{\rho_l}) \]</div> <p>At the same time however, we find:</p> <div>\[ \Delta_{fast}(v+\epsilon^2 u)_\xi=\int (v+\epsilon^2 u)_{\xi\xi}=\epsilon^4\hat\lambda\int u_0 \]</div> <p>So comparing jumps, we end up with:</p> <div>\[ \epsilon^4\hat\lambda\int u_0 = -\hat\lambda\frac{\int u_0^2}{\int f_V^*u_0 d\xi}\epsilon^2\sqrt{\hat\lambda}(\sqrt{\rho_r}+\sqrt{\rho_l}) \]</div> <p>and after some simplifying:</p> <div>\[ \int u_0 = -\frac{\int u_0^2}{\int f_V^*u_0 d\xi}\sqrt{\hat\lambda}(\sqrt{\rho_r}+\sqrt{\rho_l}) \]</div> <p>So then we only get solutions with positive, order \(\epsilon^2\) \(\lambda\) when:</p> <div>\[ \int u_0 d\xi\int f_V^*u_0 d\xi&lt;0 \]</div>]]></content><author><name></name></author><summary type="html"><![CDATA[We start with the set of equations: \[\begin{aligned} U_t &amp;= \epsilon^2U_{xx}+f(U,V)\\ V_t &amp;= V_{xx}-f(U,V) \end{aligned} \] By previous analysis, we know that we get a 1 parameter family in \(K\) of pulses and fronts described by a Hamiltonian system in \(U\), for which there is a specific \(K^*\) for which we obtain a front solution. Take this \(K^*\) and the front \((U^*,V^*)=(U^*,K^*-\epsilon^2 U^*)\) that comes with it. We can expand these values and obtain: \[ \begin{aligned} K^*&amp;=K^*_0+\epsilon^2K_1^*+\mathcal O(\epsilon^4)\\ U^*&amp;=U^*_0+\epsilon^2U_1^*+\mathcal O(\epsilon^4)\\ V^*&amp;=V^*_0+\epsilon^2V_1^*+\mathcal O(\epsilon^4) = K^*_0+\epsilon^2 K_1^*-\epsilon^2 U_0^*+\mathcal O(\epsilon^4) \end{aligned} \] Let’s perturb the system to check stability, take \(\delta\) smaller than all system scales, and define: \[\begin{aligned} U(\xi) &amp;= U^*(\xi)+\delta \exp(\lambda t)u(\xi)\\ V(\xi) &amp;= V^*(\xi)+\delta \exp(\lambda t)v(\xi) \end{aligned}\] Inserting this into the PDE and throwing out terms of order \(\delta^2\), we get the linearized equations: \[ \begin{aligned} \lambda u &amp;= \epsilon^2u_{xx}+f_U(U^*,V^*)u+f_V(U^*,V^*)v\\ \lambda v&amp;= v_{xx}-f_U(U^*,V^*)u-f_V(U^*,V^*)v \end{aligned} \]]]></summary></entry></feed>